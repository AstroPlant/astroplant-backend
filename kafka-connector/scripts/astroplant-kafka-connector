#!/usr/bin/env python

import logging
import os
import click

import astroplant_database.specification as d 

from astroplant_kafka_connector.connector import run_connector

def utc_from_millis(t):
    return datetime.datetime.fromtimestamp(t / 1000, datetime.timezone.utc)


@click.group()
def cli():
    pass


@cli.command()
@click.option(
    "-s",
    "--stream",
    "stream_type",
    default="aggregate",
    show_default=True,
    type=click.Choice(["raw", "aggregate"]),
)
def run(stream_type):
    """
    Run the measurements connector.
    :param stream: Which measurements stream to consume from Kafka and input to
    the database.
    """
    from kafka import KafkaConsumer

    logger.info(f"Running {stream_type} connector.")

    logger.debug("Creating Kafka consumer.")
    kafka_host = os.environ.get("KAFKA_HOST", "localhost")
    kafka_port = int(os.environ.get("KAFKA_PORT", "9092"))
    kafka_username = os.environ.get("KAFKA_USERNAME")
    kafka_password = os.environ.get("KAFKA_PASSWORD")
    kafka_consumer_group = os.environ.get("KAFKA_CONSUMER_GROUP", "astroplant-kafka-connector")

    logger.info(f"Kafka bootstrapping to {kafka_host}:{kafka_port}.")
    logger.info(f"Kafka consumer group: {kafka_consumer_group}.")

    # Kafka consumer configuration
    # - Consume earliest available message.
    # - It is recommended to set the offest manually after the message has been processed.
    # - Authentication is optional.
    kafka_consumer = KafkaConsumer(
        f"{stream_type}",
        bootstrap_servers=[f"{kafka_host}:{kafka_port}"],
        group_id=kafka_consumer_group,
        auto_offset_reset="earliest",
        enable_auto_commit=False,
        security_protocol="SASL_PLAINTEXT" if kafka_username else "PLAINTEXT",
        sasl_mechanism="PLAIN" if kafka_username else None,
        sasl_plain_username=kafka_username,
        sasl_plain_password=kafka_password,
    )

    db = d.DatabaseManager(
        os.environ.get(
            "DATABASE_URL",
            "postgres+psycopg2://astroplant:astroplant@localhost/astroplant",
        )
    )
    run_connector(db, kafka_consumer, stream_type)


if __name__ == "__main__":
    logger = logging.getLogger("astroplant_kafka_connector")
    logger.setLevel(logging.DEBUG)

    ch = logging.StreamHandler()
    ch.setLevel(logging.getLevelName(os.environ.get("LOG_LEVEL", "INFO")))

    formatter = logging.Formatter(
        "%(asctime)s - %(threadName)s - %(name)s - %(levelname)s - %(message)s"
    )

    ch.setFormatter(formatter)
    logger.addHandler(ch)

    cli()
